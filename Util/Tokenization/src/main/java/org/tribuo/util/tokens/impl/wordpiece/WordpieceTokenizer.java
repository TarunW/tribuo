package org.tribuo.util.tokens.impl.wordpiece;

import java.text.Normalizer;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;
import java.util.List;
import java.util.Set;

import org.tribuo.util.tokens.Token;
import org.tribuo.util.tokens.Token.TokenType;
import org.tribuo.util.tokens.Tokenizer;

import com.oracle.labs.mlrg.olcut.config.Config;
import com.oracle.labs.mlrg.olcut.provenance.ConfiguredObjectProvenance;
import com.oracle.labs.mlrg.olcut.provenance.impl.ConfiguredObjectProvenanceImpl;

public class WordpieceTokenizer implements Tokenizer {

    @Config
    private Wordpiece wordpiece;
    @Config
    private boolean toLowerCase = true;
    @Config
    private Tokenizer tokenizer = new WordpiecePreprocessTokenizer();
    @Config
    private boolean stripAccents = true;
    @Config
    private Set<String> neverSplitTokens = Collections.emptySet();

    // internal state member variables
    private boolean reset;

    private Token currentToken;

    private List<Token> currentWordpieceTokens = new ArrayList<>();

    private int currentWordpieceIndex;

    // for OLCUT
    @SuppressWarnings("unused")
    private WordpieceTokenizer() {
    }

    /**
     * @param wordpiece        an instance of {@link Wordpiece}
     * @param tokenizer        Wordpiece is run on the tokens generated by the
     *                         tokenizer provided here.
     * @param toLowerCase      determines whether or not to lowercase each token
     *                         before running Wordpiece on it
     * @param stripAccents     determines whether or not to strip out accents from
     *                         each token before running Wordpiece on it
     * @param neverSplitTokens a set of token values that we will not apply
     *                         Wordpiece to. This tokenizer does not coordinate with
     *                         the passed in tokenizer to make sure that it will not
     *                         split up the values in neverSplitTokens. You must
     *                         provide a tokenizer that will not split them up.
     */
    public WordpieceTokenizer(Wordpiece wordpiece, Tokenizer tokenizer, boolean toLowerCase, boolean stripAccents,
            Set<String> neverSplit) {
        this.wordpiece = wordpiece;
        this.tokenizer = tokenizer;
        this.toLowerCase = toLowerCase;
        this.stripAccents = stripAccents;
        this.neverSplitTokens = neverSplit;
    }

    @Override
    public ConfiguredObjectProvenance getProvenance() {
        return new ConfiguredObjectProvenanceImpl(this, "Tokenizer");
    }

    public static List<String> whitespaceTokenize(String text) {
        if (text.isEmpty()) {
            return Collections.emptyList();
        }
        return Arrays.asList(text.split("\\s+"));
    }

    @Override
    public void reset(CharSequence cs) {
        this.reset = true;
        this.tokenizer.reset(cs);
        this.currentWordpieceTokens.clear();
        currentWordpieceIndex = -1;
        if (this.tokenizer.advance()) {
            this.currentToken = this.tokenizer.getToken();
            getWordpieceTokens();
        }
    }

    @Override
    public boolean advance() {
        if (!reset) {
            throw new IllegalStateException("WordpieceTokenizer has not been reset.");
        }
        currentWordpieceIndex++;
        if (currentWordpieceIndex < currentWordpieceTokens.size()) {
            return true;
        } else if (tokenizer.advance()) {
            currentToken = this.tokenizer.getToken();
            getWordpieceTokens();
            currentWordpieceIndex = 0;
            if (currentWordpieceTokens.size() == 0) {
                return advance();
            }
            return true;
        } else {
            return false;
        }
    }

    private static String normalize(String text) {
        text = Normalizer.normalize(text, Normalizer.Form.NFD);
        text = text.replaceAll("\\p{Mn}", "");
        return text;
    }

    private void getWordpieceTokens() {
        this.currentWordpieceTokens.clear();

        String text = currentToken.text;

        if (neverSplitTokens.contains(text)) {
            currentWordpieceTokens.add(currentToken);
        }

        if (toLowerCase) {
            text = currentToken.text.toLowerCase();
        }

        if (this.stripAccents) {
            text = normalize(text);
        }

        List<String> wordpieces = wordpiece.wordpiece(text);

        if (wordpieces.size() == 0) {
            return;
        }

        if (wordpieces.size() == 1) {
            String wp = wordpieces.get(0);
            if (wp.equals(this.wordpiece.getUnknownToken())) {
                currentWordpieceTokens.add(new Token(wp, currentToken.start, currentToken.end, TokenType.UNKNOWN));
            } else {
                currentWordpieceTokens.add(new Token(wp, currentToken.start, currentToken.end, TokenType.WORD));
            }
            return;
        }

        int begin = currentToken.start;
        for (String wp : wordpieces) {
            TokenType type = TokenType.PREFIX;
            int end = begin + wp.length();
            if (wp.startsWith("##")) {
                end -= 2;
                type = TokenType.SUFFIX;
            }
            currentWordpieceTokens.add(new Token(wp, begin, end, type));
            begin = end;
        }
    }

    @Override
    public Token getToken() {
        if (currentWordpieceIndex < currentWordpieceTokens.size()) {
            return currentWordpieceTokens.get(currentWordpieceIndex);
        } else {
            throw new IllegalStateException("WordpieceTokenizer is not ready.");
        }
    }

    @Override
    public String getText() {
        return getToken().text;
    }

    @Override
    public int getStart() {
        return getToken().start;
    }

    @Override
    public int getEnd() {
        return getToken().end;
    }

    @Override
    public TokenType getType() {
        return getToken().type;
    }

    // TODO!
    @Override
    public WordpieceTokenizer clone() {
        throw new RuntimeException(new CloneNotSupportedException());
//      try {
//          WordpieceTokenizer copy = (WordpieceTokenizer) super.clone();
//          return copy;
//      } catch (CloneNotSupportedException e) {
//          throw new AssertionError("WordpieceTokenizer is Cloneable, but clone call failed");
//      }
    }
}